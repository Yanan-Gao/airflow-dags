from datetime import timedelta, datetime
from ttd.el_dorado.v2.base import TtdDag
from ttd.tasks.op import OpTask
from ttd.ttdenv import TtdEnvFactory
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.base_hook import BaseHook
import vertica_python
import logging

conn_params = {
    'vertica_ops': {
        # autogenerated session label by default,
        'session_label': 'dpsr:metrics:queryparse',
        # default throw error on invalid UTF-8 results
        'unicode_error': 'strict',
        # SSL is disabled by default
        'ssl': False,
        # autocommit is off by default
        'autocommit': True,
        # using server-side prepared statements is disabled by default
        'use_prepared_statements': False,
        # connection timeout is not enabled by default
        # 30 seconds timeout for a socket operation (Establishing a TCP connection or read/write operation)
        'connection_timeout': 150,
        'connection_load_balance': True,
        'binary_transfer': False
    }
}

alarm_slack_channel = '#scrum-dp-rpts-alerts'
dpsr_query_label_parser = 'dpsr-query-label-parser'
daily_hour = 5

default_args = {
    'owner': 'DPRPTS',
}


def get_db_connection(conn_name):
    if conn_name not in conn_params:
        raise ValueError(f"Unknown connection name: {conn_name}")
    conn_info = BaseHook.get_connection(conn_name)
    conn_params[conn_name]['host'] = conn_info.host
    conn_params[conn_name]['port'] = conn_info.port
    conn_params[conn_name]['user'] = conn_info.login
    conn_params[conn_name]['password'] = conn_info.password
    conn_params[conn_name]['database'] = conn_info.schema
    return conn_params[conn_name]


truncate_ready = """
truncate table ttd_dpsr.metrics_ParsedQueryStatsReady{TableSuffix};
"""

# all data for this and earlier epoch was already retrieved
# this table is assumed to be never empty - receiving `null` from
# this query will invalidate DAG run and will requier manual intervention
max_epoch_ended = """
select max(RecordEpoch) from ttd_dpsr.metrics_ParsedQueryStatsEnded{TableSuffix};
"""

# retrieve from issued and completed all records driven by the
# last previousl retrieved epoch, taken from previous query
ended_step = """
insert into ttd_dpsr.metrics_ParsedQueryStatsEnded{TableSuffix} (
    ScheduleExecutionId,
    ResultSetId,
    TimeStart,
    TimeEnd,
    ScheduleId,
    PTGId,
    Cluster,
    Subcluster,
    InitiatorNode,
    SessionId,
    TransactionId,
    StatementId,
    ResultRowCount,
    IsSuccess,
    IsRetry,
    IsResourceIntensive,
    DurationMS,
    RecordEpoch
)
select
    case
      when length(split_part(dridw.label,';',6)) > 0
          then split_part(dridw.label,';',6)::INTEGER
      else 0
    end as ScheduleExecutionId,
    case
      when length(split_part(dridw.label,';',3)) > 0
          then split_part(dridw.label,';',3)::INTEGER
      else 0
    end as ResultSetId,
    dridw."time" as time_start,
    drc2."time" as time_end,
    case
      when length(split_part(dridw.label,';',5)) > 0
        then split_part(dridw.label,';',5)::INTEGER
      else 0
    end as ScheduleId,
    case
      when length(split_part(dridw.label,';',13)) > 0
          then split_part(dridw.label,';',13)::INTEGER
      else 0
    end as PTGId,
    drc2._uuid,
    dn.subcluster_name,
    drc2.node_name,
    drc2.session_id,
    drc2.transaction_id,
    drc2.statement_id,
    drc2.processed_row_count,
    drc2.success,
    dridw.is_retry,
    case
      when split_part(dridw.label,';',16) = 'True' or split_part(dridw.label,';',16) = '1'
          then 1
      else 0
    end as IsResourceIntensive,
    datediff('ms', dridw."time", drc2."time"),
    drc2.epoch
from dcschema.dc_requests_completed drc2
join dcschema.dc_requests_issued_dw dridw
    using (_uuid, statement_id, transaction_id, session_id)
join dcschema.dc_nodes dn
    on dn.cluster_id = drc2._uuid
    and dn.node_name = drc2.node_name
where drc2.epoch > {MaxLastEpoch}
  and drc2.user_name = 'ttd_taskservice'
  and drc2.command_tag = 'SELECT'
  and dridw.label like 'v2%'
;
"""

# all data for this and earlier epoch was already retrieved
# this table is assumed to be never empty - receiving `null` from
# this query will invalidate DAG run and will requier manual intervention
max_epoch_summs = """
select max(RecordEpoch) from ttd_dpsr.metrics_ParsedQueryStatsSumms{TableSuffix};
"""

# retrieve from summaries aggregated stats for records driven by the
# last previousl retrieved epoch, taken from previous query
summs_step = """
insert into ttd_dpsr.metrics_ParsedQueryStatsSumms{TableSuffix} (
    Cluster,
    SessionId,
    TransactionId,
    StatementId,
    TheTime,
    ResourcePool,
    PeakMemoryKB,
    InputRowsProcessed,
    BytesSpilled,
    DataBytesRead,
    NetworkBytesSent,
    RecordEpoch
)
select
    des._uuid,
    des.session_id,
    des.transaction_id,
    des.statement_id,
    max(des.time),
    max(des.res_pool) AS res_pool,
    max(des.peak_memory_kb) AS peak_memory_kb,
    sum(des.input_rows_processed) AS input_rows_processed,
    sum(des.bytes_spilled) AS bytes_spilled,
    sum(des.data_bytes_read) AS data_bytes_read,
    sum(des.network_bytes_sent) AS network_bytes_sent,
    max(des.epoch)
from dcschema.dc_execution_summaries des
where des.res_pool in ('extract', 'extract_spill')
  and des.epoch > {MaxLastEpoch}
  and des.user_name = 'ttd_taskservice'
  and (des._uuid, des.session_id, des.transaction_id, des.statement_id) not in (
    select Cluster, SessionId, TransactionId, StatementId
    from ttd_dpsr.metrics_ParsedQueryStatsSumms{TableSuffix}
  )
group by
    des._uuid,
    des.session_id,
    des.transaction_id,
    des.statement_id
;
"""

# all data for this and earlier epoch was already retrieved
# this table is assumed to be never empty - receiving `null` from
# this query will invalidate DAG run and will requier manual intervention
max_epoch_wait = """
select max(RecordEpoch) from ttd_dpsr.metrics_ParsedQueryStatsWait{TableSuffix};
"""

wait_step = """
insert into ttd_dpsr.metrics_ParsedQueryStatsWait{TableSuffix} (
    SessionId,
    TransactionId,
    StatementId,
    Cluster,
    TimeWait,
    WaitTimeMS,
    RecordEpoch
)
select
    dcrr.session_id,
    dcrr.transaction_id,
    dcrr.statement_id,
    dcrr._uuid,
    max(dcrr.time) as TimeWait,
    max(datediff('ms', dcrr.queue_time, dcrr.acquire_time)) as WaitMsec,
    max(dcrr.epoch)
from dcschema.dc_resource_releases dcrr
where dcrr.user_name = 'ttd_taskservice'
  and dcrr.epoch > {MaxLastEpoch}
  and (dcrr.session_id, dcrr.transaction_id, dcrr.statement_id, dcrr._uuid) not in (
    select SessionId, TransactionId, StatementId, Cluster
    from ttd_dpsr.metrics_ParsedQueryStatsWait{TableSuffix}
  )
group by
    dcrr.session_id,
    dcrr.transaction_id,
    dcrr.statement_id,
    dcrr._uuid
;
"""

# Insert what we have so far in all our staged tables
ready_step = """
insert into ttd_dpsr.metrics_ParsedQueryStatsReady{TableSuffix} (
    ScheduleExecutionId,
    ResultSetId,
    TimeStart,
    TimeEnd,
    ScheduleId,
    PTGId,
    Cluster,
    Subcluster,
    InitiatorNode,
    SessionId,
    TransactionId,
    StatementId,
    ResultRowCount,
    IsSuccess,
    IsRetry,
    ResourcePool,
    PeakMemoryKB,
    InputRowsProcessed,
    BytesSpilled,
    DataBytesRead,
    NetworkBytesSent,
    IsResourceIntensive,
    WaitTimeMS,
    DurationMS,
    DurationClass
)
select
    mepqse.ScheduleExecutionId,
    mepqse.ResultSetId,
    mepqse.TimeStart,
    mepqse.TimeEnd,
    mepqse.ScheduleId,
    mepqse.PTGId,
    mepqse.Cluster,
    mepqse.Subcluster,
    mepqse.InitiatorNode,
    mepqse.SessionId,
    mepqse.TransactionId,
    mepqse.StatementId,
    mepqse.ResultRowCount,
    mepqse.IsSuccess,
    mepqse.IsRetry,
    mepqss.ResourcePool,
    mepqss.PeakMemoryKB,
    mepqss.InputRowsProcessed,
    mepqss.BytesSpilled,
    mepqss.DataBytesRead,
    mepqss.NetworkBytesSent,
    mepqse.IsResourceIntensive,
    mepqsw.WaitTimeMS,
    mepqse.DurationMS,
    ceil(log(mepqse.DurationMS/1000 + 1) * 10) as DurationClass
from ttd_dpsr.metrics_ParsedQueryStatsEnded{TableSuffix} mepqse
join ttd_dpsr.metrics_ParsedQueryStatsSumms{TableSuffix} mepqss
    on mepqse.Cluster = mepqss.Cluster
    and mepqse.SessionId = mepqss.SessionId
    and mepqse.TransactionId = mepqss.TransactionId
    and mepqse.StatementId = mepqss.StatementId
join ttd_dpsr.metrics_ParsedQueryStatsWait{TableSuffix} mepqsw
    on mepqse.Cluster = mepqsw.Cluster
    and mepqse.SessionId = mepqsw.SessionId
    and mepqse.TransactionId = mepqsw.TransactionId
    and mepqse.StatementId = mepqsw.StatementId
where
    (ScheduleExecutionId, ResultSetId, TimeStart) not in (
        select ScheduleExecutionId, ResultSetId, TimeStart
        from ttd_dpsr.metrics_ParsedQueryStats{TableSuffix}
    )
;
"""

insert_step = """
insert into ttd_dpsr.metrics_ParsedQueryStats{TableSuffix}
select * from ttd_dpsr.metrics_ParsedQueryStatsReady{TableSuffix}
;
"""

# Insert what we are going to drop based on the presence in a
# partition we are going to drop from Ended stage table.
# Note, that Ended is partitioned by TimeStart, which is predating
# in Stage and Wait times. So the Ended partition is dropped before Stage and Wait
not_ready_step = """
insert into ttd_dpsr.metrics_ParsedQueryStatsReady{TableSuffix} (
    ScheduleExecutionId,
    ResultSetId,
    TimeStart,
    TimeEnd,
    ScheduleId,
    PTGId,
    Cluster,
    Subcluster,
    InitiatorNode,
    SessionId,
    TransactionId,
    StatementId,
    ResultRowCount,
    IsSuccess,
    IsRetry,
    ResourcePool,
    PeakMemoryKB,
    InputRowsProcessed,
    BytesSpilled,
    DataBytesRead,
    NetworkBytesSent,
    IsResourceIntensive,
    WaitTimeMS,
    DurationMS,
    DurationClass
)
select
    mepqse.ScheduleExecutionId,
    mepqse.ResultSetId,
    mepqse.TimeStart,
    mepqse.TimeEnd,
    mepqse.ScheduleId,
    mepqse.PTGId,
    mepqse.Cluster,
    mepqse.Subcluster,
    mepqse.InitiatorNode,
    mepqse.SessionId,
    mepqse.TransactionId,
    mepqse.StatementId,
    mepqse.ResultRowCount,
    mepqse.IsSuccess,
    mepqse.IsRetry,
    isnull(mepqss.ResourcePool, 'unknown'),
    isnull(mepqss.PeakMemoryKB, -1),
    isnull(mepqss.InputRowsProcessed, -1),
    isnull(mepqss.BytesSpilled, -1),
    isnull(mepqss.DataBytesRead, -1),
    isnull(mepqss.NetworkBytesSent, -1),
    mepqse.IsResourceIntensive,
    isnull(mepqsw.WaitTimeMS, -1),
    mepqse.DurationMS,
    ceil(log(mepqse.DurationMS/1000 + 1) * 10) as DurationClass
from ttd_dpsr.metrics_ParsedQueryStatsEnded{TableSuffix} mepqse
left join ttd_dpsr.metrics_ParsedQueryStatsSumms{TableSuffix} mepqss
    on mepqse.Cluster = mepqss.Cluster
    and mepqse.SessionId = mepqss.SessionId
    and mepqse.TransactionId = mepqss.TransactionId
    and mepqse.StatementId = mepqss.StatementId
left join ttd_dpsr.metrics_ParsedQueryStatsWait{TableSuffix} mepqsw
    on mepqse.Cluster = mepqsw.Cluster
    and mepqse.SessionId = mepqsw.SessionId
    and mepqse.TransactionId = mepqsw.TransactionId
    and mepqse.StatementId = mepqsw.StatementId
where mepqse.TimeStart >= '{PartitionStartTime}'
  and mepqse.TimeStart < '{PartitionEndTime}'
  and (ScheduleExecutionId, ResultSetId, TimeStart) not in (
    select ScheduleExecutionId, ResultSetId, TimeStart
    from ttd_dpsr.metrics_ParsedQueryStats{TableSuffix} mpqs
    where mpqs.TimeStart >= '{PartitionStartTime}'
      and mpqs.TimeStart < '{PartitionEndTime}'
  )
;
"""

prune_old_partitions_ended = """
SELECT DROP_PARTITIONS('ttd_dpsr.metrics_ParsedQueryStatsEnded{TableSuffix}', '{KeyRangeStart}', '{KeyRangeEnd}');
"""

prune_old_partitions_summs = """
SELECT DROP_PARTITIONS('ttd_dpsr.metrics_ParsedQueryStatsSumms{TableSuffix}', '{KeyRangeStart}', '{KeyRangeEnd}');
"""

prune_old_partitions_wait = """
SELECT DROP_PARTITIONS('ttd_dpsr.metrics_ParsedQueryStatsWait{TableSuffix}', '{KeyRangeStart}', '{KeyRangeEnd}');
"""


def parse_query_labels(ti):
    is_prod = True if TtdEnvFactory.get_from_system() == TtdEnvFactory.prod else False
    dag_run = ti.get_dagrun()
    dag_end = dag_run.data_interval_end
    min_remn = dag_end.minute % 10
    # for maintanability clamp date range to the 10 minutes border
    interval_start = dag_end - timedelta(minutes=20 + min_remn, seconds=dag_end.second)
    interval_end = dag_end - timedelta(minutes=min_remn, seconds=dag_end.second)
    start_date = interval_start.strftime('%Y-%m-%d %H:%M:%S')
    end_date = interval_end.strftime('%Y-%m-%d %H:%M:%S')
    if is_prod:
        table_suffix = ""
    else:
        table_suffix = "Test"
    logging.debug("Creating connection to OPS Vertica")
    opsvert_conn_info = get_db_connection('vertica_ops')
    with vertica_python.connect(**opsvert_conn_info) as conn_vert:
        with conn_vert.cursor() as cursor_vert:
            truncate_sql_ready = truncate_ready.format(TableSuffix=table_suffix)
            # Truncate staged receiver
            logging.info(f"Executing truncate ready table for prod=={is_prod}")
            cursor_vert.execute(truncate_sql_ready)
            logging.info('Executed truncate ready table')

            # Select last epoch processed in Ended stage table and fill records
            # arrived after that last recorded epoch
            logging.info(f"Retrieving last processed epoch for ended for prod=={is_prod}")
            cursor_vert.execute(max_epoch_ended.format(TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Retrieved last processed epoch for ended: {}'.format(result))
            max_epoch_ended_value = result[0]
            logging.info(f"Executing ended step from {max_epoch_ended_value} for prod=={is_prod}")
            cursor_vert.execute(ended_step.format(MaxLastEpoch=max_epoch_ended_value, TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Executed ended step with result {}'.format(result))

            # Select last epoch processed in Summs stage table and fill records
            # arrived after that last recorded epoch
            logging.info(f"Retrieving last processed epoch for summs for prod=={is_prod}")
            cursor_vert.execute(max_epoch_summs.format(TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Retrieved last processed epoch for summs: {}'.format(result))
            max_epoch_summs_value = result[0]
            logging.info(f"Executing stage step from {max_epoch_summs_value} for prod=={is_prod}")
            cursor_vert.execute(summs_step.format(MaxLastEpoch=max_epoch_summs_value, TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Executed summs step with result {}'.format(result))

            # Select last epoch processed in Wait stage table and fill records
            # arrived after that last recorded epoch
            logging.info(f"Retrieving last processed epoch for wait for prod=={is_prod}")
            cursor_vert.execute(max_epoch_wait.format(TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Retrieved last processed epoch for wait: {}'.format(result))
            max_epoch_wait_value = result[0]
            logging.info(f"Executing wait step from {max_epoch_wait_value} for prod=={is_prod}")
            cursor_vert.execute(wait_step.format(MaxLastEpoch=max_epoch_wait_value, TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Executed wait step with result {}'.format(result))

            # Extract all ready information - using inner join here to extract information
            # for queryes where all stage tables have pieces ready
            logging.info(f"Executing ready step for prod=={is_prod}")
            cursor_vert.execute(ready_step.format(TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Executed ready step with result {}'.format(result))

            # Insert retrieved information into target long store table
            logging.info(f"Executing insert step for prod=={is_prod}")
            cursor_vert.execute(insert_step.format(TableSuffix=table_suffix))
            result = cursor_vert.fetchone()
            logging.info('Executed insert step with result {}'.format(result))

            # If this ended new hour - the end has zero minutes
            # 1. Insert from Ended stage for the partition to be dropped whatever we have
            # 2. Prune oldest partition from all stage tables
            if interval_end.minute == 0:
                old_key_time = interval_end - timedelta(hours=8)
                old_key = old_key_time.strftime('%Y-%m-%d %H:00:00')
                next_key_time = interval_end - timedelta(hours=7)
                next_key = next_key_time.strftime('%Y-%m-%d %H:00:00')

                # Truncate staged receiver
                logging.info(f"Executing truncate ready table for prod=={is_prod}")
                cursor_vert.execute(truncate_sql_ready)
                logging.info('Executed truncate ready table')

                # Save what we have with -1/empty markers for absent values
                logging.info(f"Saving what we have for time range from {old_key}  to {next_key} for prod=={is_prod}")
                cursor_vert.execute(not_ready_step.format(PartitionStartTime=old_key, PartitionEndTime=next_key, TableSuffix=table_suffix))
                result = cursor_vert.fetchone()
                logging.info('Executed saving partial from stage tables with result {}'.format(result))

                # Now drop oldest partitions
                prune_sql_ended = prune_old_partitions_ended.format(KeyRangeStart=old_key, KeyRangeEnd=old_key, TableSuffix=table_suffix)
                logging.info(f"Executing pruning ended table for key {old_key} for prod=={is_prod}")
                cursor_vert.execute(prune_sql_ended)
                result = cursor_vert.fetchone()
                logging.info('Executed pruning ended table with result {}'.format(result))
                prune_sql_summs = prune_old_partitions_summs.format(KeyRangeStart=old_key, KeyRangeEnd=old_key, TableSuffix=table_suffix)
                logging.info(f"Executing pruning summs table for key {old_key} for prod=={is_prod}")
                cursor_vert.execute(prune_sql_summs)
                result = cursor_vert.fetchone()
                logging.info('Executed pruning summs table with result {}'.format(result))
                prune_sql_wait = prune_old_partitions_wait.format(KeyRangeStart=old_key, KeyRangeEnd=old_key, TableSuffix=table_suffix)
                logging.info(f"Executing pruning wait table for key {old_key} for prod=={is_prod}")
                cursor_vert.execute(prune_sql_wait)
                result = cursor_vert.fetchone()
                logging.info('Executed pruning wait table with result {}'.format(result))


dag = TtdDag(
    dag_id=dpsr_query_label_parser,
    default_args=default_args,
    dagrun_timeout=timedelta(minutes=3),
    slack_channel=alarm_slack_channel,
    schedule_interval='7,17,27,37,47,57 * * * *',  # Try to schedule in least busy time
    max_active_runs=1,
    run_only_latest=True,  # transforms into Airflow.DAG.catchup=False
    depends_on_past=True,
    start_date=datetime(2024, 12, 20, 20, 30, 0),
    tags=['DPRPTS', 'DPSRInfra', 'OPSLabelParser'],
)

adag = dag.airflow_dag

# DAG Task Flow
do_query_labels = OpTask(op=PythonOperator(task_id="parse_query_labels", python_callable=parse_query_labels))

dag >> do_query_labels
