import logging
import pymssql
import vertica_python
import importlib
from timeit import default_timer as timer
from airflow.hooks.base_hook import BaseHook

batch_lines = 10000

conn_params = {
    'vertica_ops': {
        # autogenerated session label by default,
        'session_label': 'dpsr:metrics:repopulate',
        # default throw error on invalid UTF-8 results
        'unicode_error': 'strict',
        # SSL is disabled by default
        'ssl': False,
        # autocommit is off by default
        'autocommit': True,
        # using server-side prepared statements is disabled by default
        'use_prepared_statements': False,
        # connection timeout is not enabled by default
        # 30 seconds timeout for a socket operation (Establishing a TCP connection or read/write operation)
        'connection_timeout': 150,
        'connection_load_balance': True,
        'binary_transfer': False
    },
    'provdb_bi_metric': {
        'appname': 'DPSR Metrics exporter',
        'read_only': True,
        'arraysize': batch_lines  # fetch this many rows at once with fetchmany()
    },
    'ttdglobal_int_metric': {
        'appname': 'DPSR Metrics exporter',
        'read_only': True,
        'arraysize': batch_lines  # fetch this many rows at once with fetchmany()
    }
}


def get_db_connection(conn_name):
    if not has_key(conn_params, conn_name):
        raise ValueError(f"Unknown connection name: {conn_name}")
    conn_info = BaseHook.get_connection(conn_name)
    conn_params[conn_name]['host'] = conn_info.host
    conn_params[conn_name]['port'] = conn_info.port
    conn_params[conn_name]['user'] = conn_info.login
    conn_params[conn_name]['password'] = conn_info.password
    conn_params[conn_name]['database'] = conn_info.schema
    return conn_params[conn_name]


dates = {'Empty': None}


def transfer_data(step_name, src_sql, src_db, dst_sql, transform):
    logging.info("trying to connect")
    provdb_conn_info = get_db_connection(src_db)
    opsvert_conn_info = get_db_connection('vertica_ops')
    total = 0
    load_start = timer()
    with pymssql.connect(**provdb_conn_info) as conn_ms:
        with vertica_python.connect(**opsvert_conn_info) as conn_vert:
            with conn_ms.cursor() as cursor_ms:
                with conn_vert.cursor() as cursor_vert:
                    logging.debug('Executing statement {} on source'.format(step_name))
                    exec_start = timer()
                    cursor_ms.execute(src_sql)
                    exec_end = timer()
                    logging.debug('Executed statement in {} seconds'.format(exec_end - exec_start))

                    prev_ts = timer()
                    res_list = cursor_ms.fetchmany(batch_lines)
                    try:
                        while len(res_list) > 0:
                            if transform is not None:
                                res_list = transform(res_list)
                            if isinstance(dst_sql, list):
                                for the_part in dst_sql:
                                    cursor_vert.executemany(the_part, res_list)
                            else:
                                cursor_vert.executemany(dst_sql, res_list)
                            now_ts = timer()
                            total += len(res_list)
                            logging.debug(
                                'Processed next {} rows in {} seconds with total so far {}'
                                .format(len(res_list), (now_ts - prev_ts), total)
                            )
                            prev_ts = now_ts
                            res_list = cursor_ms.fetchmany(batch_lines)
                    except vertica_python.errors.QueryError as qerr:
                        logging.error(f'Query error occured while processing {total} lines')
                        raise

    load_end = timer()
    logging.info('Loaded {} lines in {} seconds for step {}'.format(total, (load_end - load_start), step_name))


def transfer_step(step_name, src_template, src_db, dst_template, tbl_suffix, transform):
    src_sql = src_template.format(StartDateUtcInclusive=dates['StartDateUtcInclusive'], EndDateUtcExclusive=dates['EndDateUtcExclusive'])
    if isinstance(dst_template, list):
        dst_sql = []
        for the_part in dst_template:
            dst_sql.append(
                the_part.format(
                    StartDateUtcInclusive=dates['StartDateUtcInclusive'],
                    EndDateUtcExclusive=dates['EndDateUtcExclusive'],
                    TableSuffix=tbl_suffix
                )
            )
        logging.debug('For the step {}\nthe transformed src_sql is {}\nthe transformed dst_sql is LIST'.format(step_name, src_sql))
        transfer_data(step_name, src_sql, src_db, dst_sql, transform)
    else:
        dst_sql = dst_template.format(
            StartDateUtcInclusive=dates['StartDateUtcInclusive'], EndDateUtcExclusive=dates['EndDateUtcExclusive'], TableSuffix=tbl_suffix
        )
        logging.debug('For the step {}\nthe transformed src_sql is {}\nthe transformed dst_sql is {}'.format(step_name, src_sql, dst_sql))
        transfer_data(step_name, src_sql, src_db, dst_sql, transform)


def dst_transform(step_name, dst_sql):
    logging.info("trying to connect")
    opsvert_conn_info = get_db_connection('vertica_ops')
    with vertica_python.connect(**opsvert_conn_info) as conn_vert:
        with conn_vert.cursor() as cursor_vert:
            logging.debug('Executing statement {} on destination'.format(step_name))
            exec_start = timer()
            cursor_vert.execute(dst_sql)
            result = cursor_vert.fetchone()
            exec_end = timer()
            logging.info('Executed statement in {} seconds with result {}'.format(exec_end - exec_start, result))


def transform_steps(step_name, dst_template, tbl_suffix):
    dst_sql = dst_template.format(
        StartDateUtcInclusive=dates['StartDateUtcInclusive'], EndDateUtcExclusive=dates['EndDateUtcExclusive'], TableSuffix=tbl_suffix
    )
    logging.debug('For the step {}\nthe transformed dst_sql is {}'.format(step_name, dst_sql))
    dst_transform(step_name, dst_sql)


def has_key(holder, key):
    return key in holder


def execute_step(step_name, interval_start, interval_end, is_prod):
    logging.info(f"Executing step {step_name} from {interval_start} up to {interval_end} for prod=={is_prod}")
    dates['StartDateUtcInclusive'] = interval_start.strftime('%Y-%m-%d %H:%M:%S')
    dates['EndDateUtcExclusive'] = interval_end.strftime('%Y-%m-%d %H:%M:%S')
    # See if we really have this step
    update_step = importlib.import_module('dags.dprpts.metrics.update_steps.' + step_name)
    tbl_suffix = ''
    if not is_prod:
        tbl_suffix = 'Test'
    if hasattr(update_step, 'Pre'):
        pre_step = update_step.Pre
        if isinstance(pre_step, list):
            for entry in pre_step:
                transform_steps(step_name + ':Pre', entry, tbl_suffix)
        else:
            transform_steps(step_name + ':Pre', pre_step, tbl_suffix)
    if hasattr(update_step, 'Src') and hasattr(update_step, 'Dst'):
        src_template = update_step.Src
        src_db = ''
        if hasattr(update_step, 'SrcDB'):
            src_db = update_step.SrcDB
        else:
            src_db = 'provdb_bi_metric'
        if hasattr(update_step, 'Transform'):
            transform = update_step.Transform
            if not callable(transform):
                raise ValueError(f"Transform entry for step {step_name} is not callable")
            transfer_step(step_name, src_template, src_db, update_step.Dst, tbl_suffix, transform)
        else:
            transfer_step(step_name, src_template, src_db, update_step.Dst, tbl_suffix, None)
    if hasattr(update_step, 'Post'):
        post_step = update_step.Post
        if isinstance(post_step, list):
            for entry in post_step:
                transform_steps(step_name + ':Post', entry, tbl_suffix)
        else:
            transform_steps(step_name + ':Post', post_step, tbl_suffix)
